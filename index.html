<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sunzid Hassan</title>

    <meta name="author" content="Sunzid Hassan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Sunzid Hassan
                </p>
                <p>I'm a Computational Analysis and Modelling Ph.D. candidate at Louisiana Tech University. I'm working on embodied intelligence as a research assistant at the Automatic Control Lab under the supervision of Dr. Lingxiao Wang.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sunzidhassan@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/SunzidHassan-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=OAsgmDEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sunzid-hassan/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/SunzidHassan/">Github</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/sunzid.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/sunzid.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in embodied artificial and biological intelligence.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- Project LLMOSL24 -->
    <tr onmouseout="llmOSL24_stop()" onmouseover="llmOSL24_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='llmOSL24_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/llmOSL24.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/llmOSL24.png' width="160">
        </div>
        <script type="text/javascript">
          function llmOSL24_start() {
            document.getElementById('llmOSL24_image').style.opacity = "1";
          }

          function llmOSL24_stop() {
            document.getElementById('llmOSL24_image').style.opacity = "0";
          }
          llmOSL24_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://llmOSL24.github.io/">
			<span class="papertitle">llmOSL: Integrating Vision and Olfaction via Multi-modal LLM for Robotic Odor Source Localization.
</span>
        </a>
        <br>
				<strong>Sunzid Hassan</strong>,
				<a href="https://ruiqigao.github.io/">Lingxiao Wang</a>,
        <a href="https://holynski.org/">Khan Raqib Mahmud</a>, 

        <br>
        <em>MDPI Sensors</em>, 2024
        <br>
        <a href="https://sunzidhassan.github.io/24_LLMOSL/">project page</a>
        <!-- /
        <a href="https://arxiv.org/abs/2405.14871">Preprint</a> -->
        <p></p>
        <p>
        We developed a navigation algorithm for robotic odor source localization. It includes a High-level Reasoning Module, which combines data from the robot's vision and olfactory sensors to create a multi-modal prompt. This prompt is used to query a multi-modal LLM to select the best navigation behavior. A Low-level Action Module then converts this behavior into executable control commands for the robot. Real-world experiments confirmed the effectiveness of our algorithm.
        </p>
      </td>
    </tr>

<!-- Project Fusion24 -->
<tr onmouseout="llmOSL24_stop()" onmouseover="llmOSL24_start()" bgcolor="#ffffd0">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='llmOSL24_image'><video  width=100% height=100% muted autoplay loop>
      <source src="images/llmOSL24.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
      <img src='images/llmOSL24.png' width="160">
    </div>
    <script type="text/javascript">
      function llmOSL24_start() {
        document.getElementById('llmOSL24_image').style.opacity = "1";
      }

      function llmOSL24_stop() {
        document.getElementById('llmOSL24_image').style.opacity = "0";
      }
      llmOSL24_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://llmOSL24.github.io/">
  <span class="papertitle">llmOSL: Integrating Vision and Olfaction via Multi-modal LLM for Robotic Odor Source Localization.
</span>
    </a>
    <br>
    <strong>Sunzid Hassan</strong>,
    <a href="https://ruiqigao.github.io/">Lingxiao Wang</a>,
    <a href="https://holynski.org/">Khan Raqib Mahmud</a>, 

    <br>
    <em>MDPI Sensors</em>, 2024
    <br>
    <a href="https://sunzidhassan.github.io/24_LLMOSL/">project page</a>
    <!-- /
    <a href="https://arxiv.org/abs/2405.14871">Preprint</a> -->
    <p></p>
    <p>
    We developed a navigation algorithm for robotic odor source localization. It includes a High-level Reasoning Module, which combines data from the robot's vision and olfactory sensors to create a multi-modal prompt. This prompt is used to query a multi-modal LLM to select the best navigation behavior. A Low-level Action Module then converts this behavior into executable control commands for the robot. Real-world experiments confirmed the effectiveness of our algorithm.
    </p>
  </td>
</tr>

<!-- Project LLMOSL24 -->
<tr onmouseout="llmOSL24_stop()" onmouseover="llmOSL24_start()" bgcolor="#ffffd0">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='llmOSL24_image'><video  width=100% height=100% muted autoplay loop>
      <source src="images/llmOSL24.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
      <img src='images/llmOSL24.png' width="160">
    </div>
    <script type="text/javascript">
      function llmOSL24_start() {
        document.getElementById('llmOSL24_image').style.opacity = "1";
      }

      function llmOSL24_stop() {
        document.getElementById('llmOSL24_image').style.opacity = "0";
      }
      llmOSL24_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://llmOSL24.github.io/">
  <span class="papertitle">llmOSL: Integrating Vision and Olfaction via Multi-modal LLM for Robotic Odor Source Localization.
</span>
    </a>
    <br>
    <strong>Sunzid Hassan</strong>,
    <a href="https://ruiqigao.github.io/">Lingxiao Wang</a>,
    <a href="https://holynski.org/">Khan Raqib Mahmud</a>, 

    <br>
    <em>MDPI Sensors</em>, 2024
    <br>
    <a href="https://sunzidhassan.github.io/24_LLMOSL/">project page</a>
    <!-- /
    <a href="https://arxiv.org/abs/2405.14871">Preprint</a> -->
    <p></p>
    <p>
    We developed a navigation algorithm for robotic odor source localization. It includes a High-level Reasoning Module, which combines data from the robot's vision and olfactory sensors to create a multi-modal prompt. This prompt is used to query a multi-modal LLM to select the best navigation behavior. A Low-level Action Module then converts this behavior into executable control commands for the robot. Real-world experiments confirmed the effectiveness of our algorithm.
    </p>
  </td>
</tr>


  </body>
</html>
